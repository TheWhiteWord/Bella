# Model Paths
models:
  phi4:
    path: "../../phi-4-multi/models/microsoft--Phi-4-multimodal-instruct"
    device: "cuda"
    dtype: "bfloat16"  # Updated to match Phi-4 requirements
    max_new_tokens: 512
    temperature: 0.9
    top_p: 0.9
  csm:
    path: "../csm-multi/models"
    device: "cuda"
    dtype: "bfloat16"
    max_seq_len: 4096

# Audio Settings
audio:
  # These settings will be automatically adjusted to match your device's capabilities
  sample_rate: null  # Will use device's native rate
  channels: 1        # Always mono for our use case
  format: "float32"
  silence_threshold: 0.01
  buffer_size: 2048  # Increased for better stability
  pre_buffer_duration: 0.25  # Reduced to minimize latency
  latency: "high"     # Higher latency for stability
  block_duration: 0.05  # Reduced for better responsiveness

# Pipeline Settings
pipeline:
  memory_limit_mb: 4096
  enable_cuda_optimization: true
  use_flash_attention: true
  streaming_enabled: true
  cuda_memory_fraction: 0.8
  torch_version: "2.2.1"  # Added to track torch version
  transformers_version: "4.49.0"  # Added to track transformers version
  flash_attn_version: "2.7.4.post1"  # Added for flash attention dependency

# Logging
logging:
  level: "INFO"
  file: "voice_assistant.log"
  console_output: true